{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87dbd863-7e79-480e-a1c3-8dbb42630a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data---\n",
      "\t {'Hours_Studied': [2, 4, 6, 8, 3, 7, 5, 9], 'Attendance': [50, 60, 65, 70, 55, 80, 60, 85], 'Passed': [0, 0, 1, 1, 0, 1, 0, 1]}\n",
      "DF dat---\n",
      "\t\n",
      "   Hours_Studied  Attendance  Passed\n",
      "0              2          50       0\n",
      "1              4          60       0\n",
      "2              6          65       1\n",
      "3              8          70       1\n",
      "4              3          55       0\n",
      "5              7          80       1\n",
      "6              5          60       0\n",
      "7              9          85       1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {\n",
    "    \"Hours_Studied\": [2, 4, 6, 8, 3, 7, 5, 9],\n",
    "    \"Attendance\": [50, 60, 65, 70, 55, 80, 60, 85],\n",
    "    \"Passed\": [0, 0, 1, 1, 0, 1, 0, 1]\n",
    "}\n",
    "df=pd.DataFrame(data)\n",
    "print('Original data---\\n\\t',data)\n",
    "print('DF dat---\\n\\t')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e1570b-c0e1-4746-ad4f-5630b53a939f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Hours_Studied    0\n",
      "Attendance       0\n",
      "Passed           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum().sum())\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7e669cc-8173-4600-a034-212789623610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hours_Studied    0\n",
      "Attendance       0\n",
      "Passed           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print(df.isnull().sum()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd8a7a-aea1-4936-8c9e-08d9f6418e05",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01b70241-11c7-46db-82ef-7bc72569de0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MinMaxSacler' from 'sklearn.preprocessing' (C:\\Users\\DAYANANDA S K\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\preprocessing\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#\"\"\" Feature Scalling and splitting data\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler,MinMaxSacler\n\u001b[32m      3\u001b[39m Scaler=StandardScaler()\n\u001b[32m      4\u001b[39m X_Scaler=Scaler.fit_transform(df[[\u001b[33m\"\u001b[39m\u001b[33mHours_Studied\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAttendance\u001b[39m\u001b[33m\"\u001b[39m]])\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'MinMaxSacler' from 'sklearn.preprocessing' (C:\\Users\\DAYANANDA S K\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\preprocessing\\__init__.py)"
     ]
    }
   ],
   "source": [
    "#\"\"\" Feature Scalling and splitting data\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxSacler\n",
    "Scaler=StandardScaler()\n",
    "X_Scaler=Scaler.fit_transform(df[[\"Hours_Studied\", \"Attendance\"]])\n",
    "scalar=MinMaxScaler()\n",
    "X_scaled=Scaler.fit_transform(df[[\"Hours_Studied\", \"Attendance\"]])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "std_scaler=StandardScaler()\n",
    "std_scaled=std_scaler.fit_transform(df)\n",
    "print(pd.DataFrame(std_scaled, columns=[Hours_Studied, Attendance, Passed]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d90d0097-e2ee-4013-8c39-1ebdee553e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Hours_Studied  Attendance  Passed\n",
      "0      -1.527525   -1.382503    -1.0\n",
      "1      -0.654654   -0.497701    -1.0\n",
      "2       0.218218   -0.055300     1.0\n",
      "3       1.091089    0.387101     1.0\n",
      "4      -1.091089   -0.940102    -1.0\n",
      "5       0.654654    1.271903     1.0\n",
      "6      -0.218218   -0.497701    -1.0\n",
      "7       1.527525    1.714304     1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "Scaler = StandardScaler()\n",
    "X_Scaler = Scaler.fit_transform(df[[\"Hours_Studied\", \"Attendance\"]])   # must pass feature data\n",
    "\n",
    "scalar = MinMaxScaler()\n",
    "X_scaled = scalar.fit_transform(df[[\"Hours_Studied\", \"Attendance\"]])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[[\"Hours_Studied\", \"Attendance\"]], df[\"Passed\"], test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "std_scaled = std_scaler.fit_transform(df[[\"Hours_Studied\", \"Attendance\", \"Passed\"]])\n",
    "\n",
    "print(pd.DataFrame(std_scaled, columns=[\"Hours_Studied\", \"Attendance\", \"Passed\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28dff793-7cf6-4db0-9aa3-4c330f12ed70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Hours_Studied  Attendance  Passed\n",
      "0       0.000000    0.000000     0.0\n",
      "1       0.285714    0.285714     0.0\n",
      "2       0.571429    0.428571     1.0\n",
      "3       0.857143    0.571429     1.0\n",
      "4       0.142857    0.142857     0.0\n",
      "5       0.714286    0.857143     1.0\n",
      "6       0.428571    0.285714     0.0\n",
      "7       1.000000    1.000000     1.0\n"
     ]
    }
   ],
   "source": [
    "# \"\"\" Feature Scaling and splitting data \"\"\"\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Use MinMaxScaler to keep values positive (0â€“1 range)\n",
    "scalar = MinMaxScaler()\n",
    "X_scaled = scalar.fit_transform(df[[\"Hours_Studied\", \"Attendance\", \"Passed\"]])\n",
    "# Split the data\n",
    "X_train, X_test = train_test_split(X_scaled, test_size=0.25, random_state=42)\n",
    "# Display scaled data (no negatives)\n",
    "print(pd.DataFrame(X_scaled, columns=[\"Hours_Studied\", \"Attendance\", \"Passed\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "935e716f-f9ed-4072-ad36-ec42c47893fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MinMaxSacler' from 'sklearn.preprocessing' (C:\\Users\\DAYANANDA S K\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\preprocessing\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#\"\"\" Feature Scalling and splitting data\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler,MinMaxSacler\n\u001b[32m      3\u001b[39m Scaler=StandardScaler()\n\u001b[32m      4\u001b[39m X_Scaler=Scaler.fit_transform(df[[\u001b[33m\"\u001b[39m\u001b[33mHours_Studied\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAttendance\u001b[39m\u001b[33m\"\u001b[39m]])\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'MinMaxSacler' from 'sklearn.preprocessing' (C:\\Users\\DAYANANDA S K\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\preprocessing\\__init__.py)"
     ]
    }
   ],
   "source": [
    "#\"\"\" Feature Scalling and splitting data\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxSacler\n",
    "Scaler=StandardScaler()\n",
    "X_Scaler=Scaler.fit_transform(df[[\"Hours_Studied\", \"Attendance\"]])\n",
    "scalar=MinMaxScaler()\n",
    "X_scaled=Scaler.fit_transform(df[[\"Hours_Studied\", \"Attendance\"]])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "std_scaler=StandardScaler()\n",
    "std_scaled=std_scaler.fit_transform(df)\n",
    "print(pd.DataFrame(std_scaled, columns=[Hours_Studied, Attendance, Passed]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5b33f4-2514-4502-b90d-948bb5610122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
